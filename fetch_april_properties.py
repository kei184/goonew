import os
import time
import tempfile
import traceback
import re
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

import requests
from bs4 import BeautifulSoup

import gspread
from oauth2client.service_account import ServiceAccountCredentials

# === 1. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®š ===
SPREADSHEET_ID = '1LpduIjFPimgUX6g1j5cfLnMT6OayfA5un3it2Z5rwuE'
SHEET_NAME = 'æ–°ç€ç‰©ä»¶'

# === 2. èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ ===
def create_credentials_file():
    with tempfile.NamedTemporaryFile(delete=False, suffix=".json") as tmp:
        tmp.write(os.environ['GOOGLE_CREDENTIALS_JSON'].encode())
        return tmp.name

# === 3. SUUMO å„ã‚¨ãƒªã‚¢ã®æ–°ç€ç‰©ä»¶ä¸€è¦§ã‹ã‚‰ç‰©ä»¶åã‚’å–å¾— ===
def fetch_suumo_properties():
    base_url = "https://suumo.jp"
    area_paths = [
        "/ms/shinchiku/hokkaido/",
        "/ms/shinchiku/tohoku/",
        "/ms/shinchiku/kanto/",
        "/ms/shinchiku/chubu/",
        "/ms/shinchiku/kinki/",
        "/ms/shinchiku/chugoku/",
        "/ms/shinchiku/shikoku/",
        "/ms/shinchiku/kyushu/"
    ]

    headers = {"User-Agent": "Mozilla/5.0"}
    all_props = []

    for path in area_paths:
        url = base_url + path
        res = requests.get(url, headers=headers)
        if res.status_code != 200:
            print(f"âŒ ã‚¨ãƒªã‚¢ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {url}")
            continue

        soup = BeautifulSoup(res.text, 'html.parser')
        new_link = soup.find("a", string=re.compile("ä»Šé€±ã®.*æ–°ç€ç‰©ä»¶"))
        if not new_link or not new_link.get("href"):
            print(f"âš ï¸ æ–°ç€ç‰©ä»¶ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {url}")
            continue

        list_url = base_url + new_link["href"]
        print(f"ğŸ” å–å¾—ä¸­: {list_url}")
        res_list = requests.get(list_url, headers=headers)
        if res_list.status_code != 200:
            print(f"âŒ ä¸€è¦§ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {list_url}")
            continue

        list_soup = BeautifulSoup(res_list.text, 'html.parser')
        for a in list_soup.select("a.cassette_header-title"):
            name = a.text.strip()
            if name:
                all_props.append(name)
        time.sleep(1)

    unique = list(dict.fromkeys(all_props))
    print(f"âœ… å–å¾—æ¸ˆã¿ç‰©ä»¶ï¼ˆé‡è¤‡é™¤å»ï¼‰: {len(unique)} ä»¶")
    return unique

# === 4. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸è¨˜éŒ²ï¼ˆB:ç‰©ä»¶å, C:ãƒãƒ³ã‚·ãƒ§ãƒ³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£URLï¼‰ ===
def write_to_sheet(names, cred_path):
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_name(cred_path, scope)
    client = gspread.authorize(creds)
    sheet = client.open_by_key(SPREADSHEET_ID).worksheet(SHEET_NAME)

    existing_names = sheet.col_values(2)[1:]
    today = datetime.now().strftime('%Y/%m/%d')
    new_count = 0

    for name in names:
        if name in existing_names:
            print(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé‡è¤‡ï¼‰: {name}")
            continue

        mc_url = f"https://www.e-mansion.co.jp/bbs/search/?q={name}"
        sheet.append_row([today, name, mc_url])
        new_count += 1
        time.sleep(1)

    print(f"âœ… æ–°è¦è¿½åŠ : {new_count} ä»¶")

# === 5. ãƒ¡ã‚¤ãƒ³å‡¦ç† ===
def main():
    try:
        cred = create_credentials_file()
        names = fetch_suumo_properties()
        if not names:
            print("âŒ ç‰©ä»¶ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        write_to_sheet(names, cred)
    except Exception:
        print("âŒ å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼:")
        traceback.print_exc()

if __name__ == "__main__":
    main()
